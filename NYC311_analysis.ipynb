{"cells":[
{"cell_type":"markdown","metadata":{},"source":["# NYC 311 Service Requests  Data Cleaning, Profiling, Visualization & Statistical Analysis\n\nThis notebook implements an end-to-end workflow for the NYC 311 Service Requests dataset (subset: one calendar year):\n\n- Load cleaned data from SQLite\n- Perform additional cleaning and feature engineering in Python\n- Profile the dataset (automated + manual)\n- Create visualizations\n- Run hypothesis tests, correlation analysis, and regression\n- Summarize findings, limitations, and next steps\n"]},
{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","from sqlalchemy import create_engine\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Profiling (try ydata_profiling, fall back to pandas_profiling if available)\n","try:\n","    from ydata_profiling import ProfileReport\n","except ImportError:\n","    try:\n","        from pandas_profiling import ProfileReport\n","    except ImportError:\n","        ProfileReport = None\n","        print(\"Warning: ydata_profiling / pandas_profiling not installed; profiling report will be skipped.\")\n","\n","from scipy import stats\n","import statsmodels.api as sm\n","import statsmodels.formula.api as smf\n","\n","sns.set(style='whitegrid', context='talk')\n","plt.rcParams['figure.figsize'] = (10, 4)\n"]},
{"cell_type":"markdown","metadata":{},"source":["## 1. Load data from SQLite database\n\nWe load the cleaned, deduplicated table `clean_311_2023_dedup` created by the SQL script `nyc311_sql_tasks.sql`.\n"]},
{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["db_path = 'nyc311.db'\n","engine = create_engine(f'sqlite:///{db_path}')\n","\n","query = 'SELECT * FROM clean_311_2023_dedup;'\n","df = pd.read_sql(query, engine)\n","\n","df.shape, df.head()\n"]},
{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.info()\n"]},
{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.describe(include='all').T.head(25)\n"]},
{"cell_type":"markdown","metadata":{},"source":["## 2. Data cleaning and feature engineering in Python\n\nWe normalize data types, handle timestamps, and engineer features such as response time and temporal indicators.\n"]},
{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Datetime conversion\n","df['created_datetime'] = pd.to_datetime(df['created_datetime'], errors='coerce')\n","df['closed_datetime'] = pd.to_datetime(df['closed_datetime'], errors='coerce')\n","\n","# Latitude / longitude to numeric\n","df['latitude'] = pd.to_numeric(df['latitude'], errors='coerce')\n","df['longitude'] = pd.to_numeric(df['longitude'], errors='coerce')\n","\n","# Categorical fields\n","categorical_cols = ['complaint_type', 'complaint_group', 'borough', 'status', 'agency']\n","for col in categorical_cols:\n","    if col in df.columns:\n","        df[col] = df[col].astype('category')\n","\n","df.dtypes.head(20)\n"]},
{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Missingness summary (fraction of missing per column)\n","missing_fraction = df.isna().mean().sort_values(ascending=False)\n","missing_fraction.head(20)\n"]},
{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Duplicates check by unique_key (should be 0 due to SQL dedup)\n","df.duplicated(subset=['unique_key']).sum()\n"]},
{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Feature engineering: response time and temporal features\n","df['response_time_hours'] = (df['closed_datetime'] - df['created_datetime']).dt.total_seconds() / 3600\n","\n","# Closed-only subset for response-time analysis\n","df_closed = df[df['response_time_hours'].notna()].copy()\n","\n","df['hour_of_day'] = df['created_datetime'].dt.hour\n","df['day_of_week'] = df['created_datetime'].dt.dayofweek  # 0=Mon\n","df['month'] = df['created_datetime'].dt.month\n","df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n","\n","df[['response_time_hours', 'hour_of_day', 'day_of_week', 'month', 'is_weekend']].describe()\n"]},
{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Outlier handling for response time\n","df_closed['response_time_hours'].describe()\n"]},
{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["p99 = df_closed['response_time_hours'].quantile(0.99)\n","df_closed['response_time_capped'] = df_closed['response_time_hours'].clip(upper=p99)\n","p99\n"]},
{"cell_type":"markdown","metadata":{},"source":["## 3. Profiling\n\nWe generate an automated profiling report (if profiling libraries are available), and also compute manual summaries.\n"]},
{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if ProfileReport is not None:\n","    profile = ProfileReport(df, title='NYC 311  2023 Cleaned Data Profiling Report', explorative=True)\n","    profile.to_file('nyc311_profile.html')\n","    'Saved nyc311_profile.html'\n","else:\n","    'Profiling skipped (ProfileReport not available)'\n"]},
{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df['complaint_group'].value_counts().head(10)\n"]},
{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df['borough'].value_counts(dropna=False)\n"]},
{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df['status'].value_counts(dropna=False)\n"]},
{"cell_type":"markdown","metadata":{},"source":["## 4. Visualizations\n\nWe create several core plots:\n\n1. Time series of complaint volume (monthly)\n2. Bar chart of top complaint types\n3. Geospatial scatter plot (latitude vs longitude)\n4. Distribution of response times\n"]},
{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 4.1 Time series of monthly complaint volume\n","ts_monthly = (\n","    df.set_index('created_datetime')\n","      .resample('M')['unique_key']\n","      .count()\n","      .rename('complaint_count')\n",")\n","\n","plt.figure(figsize=(10, 4))\n","sns.lineplot(x=ts_monthly.index, y=ts_monthly.values)\n","plt.title('Monthly Complaint Volume  2023')\n","plt.xlabel('Month')\n","plt.ylabel('Number of Complaints')\n","plt.tight_layout()\n","plt.show()\n"]},
{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 4.2 Bar chart of top complaint types\n","top_n = 10\n","complaints_top = df['complaint_type'].value_counts().head(top_n)\n","\n","plt.figure(figsize=(10, 5))\n","sns.barplot(x=complaints_top.values, y=complaints_top.index, orient='h')\n","plt.title(f'Top {top_n} Complaint Types  2023')\n","plt.xlabel('Number of Complaints')\n","plt.ylabel('Complaint Type')\n","plt.tight_layout()\n","plt.show()\n"]},
{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 4.3 Geospatial scatter plot (latitude vs longitude)\n","sample_size = min(50000, len(df))\n","geo_sample = df.sample(n=sample_size, random_state=42)\n","\n","plt.figure(figsize=(6, 6))\n","plt.scatter(geo_sample['longitude'], geo_sample['latitude'], s=1, alpha=0.2)\n","plt.title('Spatial Distribution of Complaints  2023')\n","plt.xlabel('Longitude')\n","plt.ylabel('Latitude')\n","plt.tight_layout()\n","plt.show()\n"]},
{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 4.4 Distribution of response times (closed complaints)\n","plt.figure(figsize=(10, 4))\n","sns.histplot(df_closed['response_time_hours'], bins=50, kde=True)\n","plt.xlim(0, p99)\n","plt.title('Distribution of Response Time (Hours)  Closed Complaints')\n","plt.xlabel('Response Time (hours)')\n","plt.ylabel('Count')\n","plt.tight_layout()\n","plt.show()\n"]},
{"cell_type":"markdown","metadata":{},"source":["## 5. Statistical Analysis\n\nWe perform:\n\n- Two hypothesis tests (difference in means, chi-square independence)\n- Correlation analysis between numeric variables\n- A regression model predicting response time\n"]},
{"cell_type":"markdown","metadata":{},"source":["### 5.1 Hypothesis Test 1: Difference in Mean Response Time between Noise and Heating Complaints\n\n- **H  (null)**: Mean response time is equal for Noise and Heating complaints.\n- **H (alternative)**: Mean response times differ between Noise and Heating complaints.\n\nWe use a two-sample t-test with unequal variances on closed complaints.\n"]},
{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["groups = ['Noise', 'Heating']\n","subset = df_closed[df_closed['complaint_group'].isin(groups)].copy()\n","\n","noise = subset[subset['complaint_group'] == 'Noise']['response_time_hours'].dropna()\n","heat  = subset[subset['complaint_group'] == 'Heating']['response_time_hours'].dropna()\n","\n","len(noise), len(heat)\n"]},
{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if len(noise) > 1 and len(heat) > 1:\n","    t_stat, p_val = stats.ttest_ind(noise, heat, equal_var=False)\n","    t_stat, p_val\n","else:\n","    'Not enough data in one or both groups for t-test'\n"]},
{"cell_type":"markdown","metadata":{},"source":["### 5.2 Hypothesis Test 2: Association between Complaint Group and Borough\n\n- **H 0 (null)**: Complaint group is independent of borough.\n- **H (alternative)**: Complaint group is associated with borough.\n\nWe use a chi-square test of independence on the contingency table of (complaint_group \u00d7 borough).\n"]},
{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ct = pd.crosstab(df['complaint_group'], df['borough'])\n","\n","if ct.shape[0] > 1 and ct.shape[1] > 1:\n","    chi2, p, dof, expected = stats.chi2_contingency(ct)\n","    chi2, p, dof\n","else:\n","    'Contingency table too small for chi-square test'\n"]},
{"cell_type":"markdown","metadata":{},"source":["### 5.3 Correlation Analysis\n\nWe compute Pearson correlations between numeric variables and visualize them in a heatmap.\n"]},
{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["num_cols = ['response_time_hours', 'hour_of_day', 'day_of_week', 'month', 'latitude', 'longitude']\n","corr_pearson = df_closed[num_cols].corr(method='pearson')\n","corr_pearson\n"]},
{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.figure(figsize=(6, 5))\n","sns.heatmap(corr_pearson, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n","plt.title('Pearson Correlation  Numeric Variables')\n","plt.tight_layout()\n","plt.show()\n"]},
{"cell_type":"markdown","metadata":{},"source":["### 5.4 Regression Model: Predicting Response Time\n\nWe model `response_time_hours` for closed complaints using:\n\n- Categorical: `complaint_group`, `borough`\n- Numeric: `hour_of_day`\n- Binary: `is_weekend`\n\nWe fit an OLS regression and interpret coefficients and model fit.\n"]},
{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_df = df_closed[['response_time_hours', 'complaint_group', 'borough', 'hour_of_day', 'is_weekend']].dropna().copy()\n","\n","if not model_df.empty:\n","    p99_model = model_df['response_time_hours'].quantile(0.99)\n","    model_df = model_df[model_df['response_time_hours'] <= p99_model]\n","\n","    formula = 'response_time_hours ~ C(complaint_group) + C(borough) + hour_of_day + is_weekend'\n","    model = smf.ols(formula=formula, data=model_df).fit()\n","    print(model.summary())\n","else:\n","    print('Not enough data to fit regression model.')\n"]},
{"cell_type":"markdown","metadata":{},"source":["## 6. Conclusions, Limitations, and Future Work\n\nSee `report_text.md` for a full narrative discussion of:\n\n- Key findings from EDA and statistical analysis\n- Data quality issues and assumptions\n- Potential biases and limitations\n- Suggestions for future extensions (dashboards, geospatial heatmaps, multi-year analysis, richer models)\n"]}
],
"metadata":{
  "kernelspec":{
    "display_name":"Python 3",
    "language":"python",
    "name":"python3"
  },
  "language_info":{
    "name":"python",
    "version":"3"
  }
},
"nbformat":4,
"nbformat_minor":5
}